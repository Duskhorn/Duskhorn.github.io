<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-01-22 Mon 18:24 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Riassunto Data and Web Mining</title>
<meta name="author" content="Duskhorn" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Riassunto Data and Web Mining</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org8dfe318">1. Workflow</a>
<ul>
<li><a href="#org9435b25">1.1. Train-test</a></li>
<li><a href="#orgb583c96">1.2. k-fold</a></li>
<li><a href="#org4767d30">1.3. Stratified sampling</a></li>
<li><a href="#orgf5f13fc">1.4. \(Error = bias^2 + Var + \varepsilon\)!</a></li>
</ul>
</li>
<li><a href="#orgb07eb3f">2. Classification</a>
<ul>
<li><a href="#org11d59f0">2.1. kNN</a></li>
<li><a href="#org17a4f3f">2.2. Decision trees</a>
<ul>
<li><a href="#org252a0e2">2.2.1. Algoritmo di Hunt</a></li>
<li><a href="#orgaee0994">2.2.2. Valutazione e selezione dei modelli</a></li>
</ul>
</li>
<li><a href="#org8195af1">2.3. Regressione logistica</a></li>
<li><a href="#orgae02931">2.4. SVM</a></li>
</ul>
</li>
<li><a href="#org4f7fa5d">3. Metodi ensemble</a>
<ul>
<li><a href="#org7010f24">3.1. Bagging</a></li>
<li><a href="#org6d26fa6">3.2. Boosting</a></li>
<li><a href="#orgac3e57f">3.3. Random Forest</a></li>
</ul>
</li>
<li><a href="#orgda7a939">4. Feature Engineering</a>
<ul>
<li><a href="#org024ab00">4.1. Encoding</a></li>
<li><a href="#org4ce966a">4.2. Misure di accuratezza</a></li>
<li><a href="#org45356e3">4.3. ROC e AUC</a></li>
</ul>
</li>
<li><a href="#org8001799">5. Text processing</a>
<ul>
<li><a href="#org9f1bc34">5.1. Text encoding</a></li>
<li><a href="#org026e67d">5.2. Bayes nativo</a></li>
</ul>
</li>
<li><a href="#org3c0ec59">6. Ranking</a>
<ul>
<li><a href="#orga32b40c">6.1. NDCG@K</a></li>
<li><a href="#org2640617">6.2. Generazione dei label</a></li>
<li><a href="#orgd470942">6.3. PageRank</a></li>
</ul>
</li>
<li><a href="#org4157f6c">7. Artificial Neural Networks</a>
<ul>
<li><a href="#orgd1cca7d">7.1. Neurone artificiale</a>
<ul>
<li><a href="#org5d881cf">7.1.1. Apprendimento</a></li>
</ul>
</li>
<li><a href="#orgaf802de">7.2. Deep Learning</a></li>
<li><a href="#org7ba668f">7.3. Overfitting</a></li>
<li><a href="#org840bc7e">7.4. Transfer learning</a></li>
<li><a href="#org10ac054">7.5. 11 Raccomandazioni</a></li>
</ul>
</li>
<li><a href="#org87930f0">8. Clustering</a>
<ul>
<li><a href="#orgdfe6040">8.1. Obbiettivi e misurazioni del clustering</a></li>
<li><a href="#orgfaed48e">8.2. Approcci di partizionamento</a>
<ul>
<li><a href="#org5c4e4a4">8.2.1. k-Means</a></li>
</ul>
</li>
<li><a href="#org697f449">8.3. Approcci gerarchici</a>
<ul>
<li><a href="#org408bec7">8.3.1. Hierarchical Agglomerative Cluster</a></li>
</ul>
</li>
<li><a href="#org769c6a7">8.4. Approcci di densità</a>
<ul>
<li><a href="#orgcf8127e">8.4.1. DBSCAN</a></li>
</ul>
</li>
<li><a href="#orgaee86ad">8.5. Valutazione dei clsuter</a>
<ul>
<li><a href="#orge1514d6">8.5.1. Intrinseca</a></li>
<li><a href="#org9ff2745">8.5.2. Estrinseca</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org8dfe318" class="outline-2">
<h2 id="org8dfe318"><span class="section-number-2">1.</span> Workflow</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org9435b25" class="outline-3">
<h3 id="org9435b25"><span class="section-number-3">1.1.</span> Train-test</h3>
</div>
<div id="outline-container-orgb583c96" class="outline-3">
<h3 id="orgb583c96"><span class="section-number-3">1.2.</span> k-fold</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Ruotare validation e training set mantenendo invariato il tuning degli iperparametri per valutare la performance del modello
</p>
</div>
</div>
<div id="outline-container-org4767d30" class="outline-3">
<h3 id="org4767d30"><span class="section-number-3">1.3.</span> Stratified sampling</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Quando si fa un sampling da un dataset per dividere in training/validation, prendere dati in modo tale da mantenere una proporzione delle classi del dataset originale
</p>
</div>
</div>
<div id="outline-container-orgf5f13fc" class="outline-3">
<h3 id="orgf5f13fc"><span class="section-number-3">1.4.</span> \(Error = bias^2 + Var + \varepsilon\)!</h3>
</div>
</div>
<div id="outline-container-orgb07eb3f" class="outline-2">
<h2 id="orgb07eb3f"><span class="section-number-2">2.</span> Classification</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org11d59f0" class="outline-3">
<h3 id="org11d59f0"><span class="section-number-3">2.1.</span> kNN</h3>
<div class="outline-text-3" id="text-2-1">
<p>
k Nearest Neighbor classifier.
</p>

<p>
A livello di training non fa altro che indicizzare i dati in input (è chiamato lazy learner).
Quando gli viene chiesto di predire un risultato, cerca i \(k\) valori più vicini al dato e conta la classe più frequente, dando quella come predizione. In particolare per \(k = \infty\) il kNN ritorna la moda tra le classi.
</p>

<p>
Ci sono vari modi di definire la distanza e dipende dal problema che si ha in mano. Esempi
</p>
<dl class="org-dl">
<dt>Minkowski : \(d_p(x,y) = \sum_i \left|x_i - y_i\right|^\frac{1}{p}\)</dt><dd>Generalizzazione delle distanze in \(\mathbb{R}^n\) (euclidea per \(p=2\), manhattan per \(p=1\));</dd>
<dt>Coseno \(d(\vec{x}, \vec{y}) = \frac{\vec{x}\cdot\vec{y}}{||\vec{a}||\cdot||\vec{b}||}\)</dt><dd>differenza di inclinazione tra due vettori, utile ad esempio in text processing;</dd>
<dt>Jaccardi \(d(A,B) = 1- \frac{|A \cap B|}{|A \cup B|}\)</dt><dd>Confronto di un attributo tra tutti i dati, simile al Coseno;</dd>
<dt>Hamming</dt><dd><p>
distanza in termini di quanti caratteri sono diversi tra due stringhe binarie. 
</p>

<p>
I kNN sono estremamente sensibili agli outliers.
</p>

<p>
Potrebbe essere utile scalare i dataset in modo tale da avere le stesse "unità di misura" per tutte le features. In particolare:
</p>
<dl class="org-dl">
<dt>Standard Scaler</dt><dd>I dati sono normalizzati in modo tale che \(x_i \sim N(\mu_i, \sigma_i^2)\) dove i parametri sono media e varianza delle features;</dd>
<dt>Min-max sclaer</dt><dd>I dati sono normalizzati in un intervallo \([0, 1]\).</dd>
</dl></dd>
</dl>
</div>
</div>

<div id="outline-container-org17a4f3f" class="outline-3">
<h3 id="org17a4f3f"><span class="section-number-3">2.2.</span> Decision trees</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Utilizzati sia per classificazione che regressione.
Si costruisce un albero che, a partire dal nodo inziale, compie delle scelte ad ogni passo fino ad arrivare ad una classificazione. Ogni nodo dell'albero corrisponde ad una scelta (binaria e non), mentre ogni foglia corrisponde a una decisione.
</p>

<p>
Costruire l'albero ottimale è un problema computazionale di classe \(NP-hard\), dunque è necesario utilizare un algoritmo greedy per costruire l'albero. La costruzione dipende dalla scelta di come calcolare il gain e l'errore.
</p>

<p>
\[Gain(Split|D) = Error(D) - \left(\frac{|D_L|}{Error(D_L)} + \frac{|D_R|}{Error(D_R)}\right)\]
Varie tipologie di errore:
</p>
<dl class="org-dl">
<dt>Errore di classificazione</dt><dd>\(Error(D) = 1 - \max_i p_i\);</dd>
<dt>Errore di informazione</dt><dd>\(Error(D) = Info(D) = \sum_ip_i\log(p_1)\);</dd>
<dt>Gain ratio</dt><dd>\(Error(D) = \frac{Info(D)}{SplitInfo(D)}\); \(SplitInfo(D) = -\sum_i\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}\);</dd>
<dt>Indice di Gini</dt><dd>\(Error(D) = 1 - \sum_i p_i\);</dd>
<dt>Regressione</dt><dd>\(Error(D) = MSE(D)\).</dd>
</dl>
</div>
<div id="outline-container-org252a0e2" class="outline-4">
<h4 id="org252a0e2"><span class="section-number-4">2.2.1.</span> Algoritmo di Hunt</h4>
<div class="outline-text-4" id="text-2-2-1">
<ol class="org-ol">
<li>A partire dalla classe più frequente, per ogni feature e per ogni threshold, esegui lo split \(S\) delle feature;</li>
<li>Calcola il \(Gain(S|D)\) per ogni split \(S\) e seleziona il migliore;</li>
<li>Se il \(Gain(S|D)\) migliore è zero, l'algoritmo è concluso;</li>
<li>Ripeti ricorsivamente per ogni split.</li>
</ol>
</div>
</div>

<div id="outline-container-orgaee0994" class="outline-4">
<h4 id="orgaee0994"><span class="section-number-4">2.2.2.</span> Valutazione e selezione dei modelli</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
Per selezionare il migliore è comodo tenere traccia anche della complessità del modello stesso nei calcoli.
\[ErrorGeneral(D, M) = Error(D, M) + \alpha\cdot Complexity(M)\]
La complessità potrebbe ad esempio essere il rapporto tra numero di foglie e il numero di nodi.
</p>

<p>
In genere: Alberi piccoli hanno bias molto alto ma varianza piccola, alberi grossi hanno bias piccolo ma varianza molto alta
</p>
</div>
</div>
</div>

<div id="outline-container-org8195af1" class="outline-3">
<h3 id="org8195af1"><span class="section-number-3">2.3.</span> Regressione logistica</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Regressione utilizzando la sigmoide come link function. Non c'è molto altro da dire
</p>
</div>
</div>

<div id="outline-container-orgae02931" class="outline-3">
<h3 id="orgae02931"><span class="section-number-3">2.4.</span> SVM</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Lo scopo è "disegnare" un iperpiano nello spazio dei punti, ottenendo così una classificazione. Gli SVM offrono solo una classificazione di tipo binario; per classificare tra una categoria non binaria, è necessario utilizzare più SVM in serie. Questo è possibile per il <a href="https://it.wikipedia.org/wiki/Teorema_del_panino_al_prosciutto">teorema del panino al prosciutto (vorrei star scherzando)</a>
</p>

<p>
In pratica è come risolvere il seguente problema di ottimizzazione:
\[\text{Minimizzare }f(w) = \frac{|w|_2^2}{2} \text{ con vincolo } g(x):\;y_i(w^Tx_i + b) \geq 1\]
Se i punti non sono separabili linearmente, è possibile introdurre un "soft margin", tramite un errore:
\[\text{Minimizzare }f(w) = \frac{|w|_2^2}{2} + C\sum_ix_i \text{ con vincolo } g(x):\;y_i(w^Tx_i + b) \geq 1 - x_i\]
</p>
</div>
</div>
</div>
<div id="outline-container-org4f7fa5d" class="outline-2">
<h2 id="org4f7fa5d"><span class="section-number-2">3.</span> Metodi ensemble</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org7010f24" class="outline-3">
<h3 id="org7010f24"><span class="section-number-3">3.1.</span> Bagging</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Metodo per ridurre la varianza del modello.
</p>
<ol class="org-ol">
<li>Dato un dataset grande \(N\), effettuare \(N\) sampling dallo stesso dataset per \(k\) volte, contemplando anche duplicati;</li>
<li>La probabilità che un dato sia presente più di una volta per \(N\) grande è \(\lim_{n\to\infty} 1 - (1 - \frac{1}{n})^n = 1 - e^{-1} \approx 63\%\);</li>
<li>Effettuare un training di \(k\) modelli in parallelo sui dataset diversi;</li>
<li>La predizione totale è la media delle predizioni per tutti i modelli.</li>
</ol>
</div>
</div>
<div id="outline-container-org6d26fa6" class="outline-3">
<h3 id="org6d26fa6"><span class="section-number-3">3.2.</span> Boosting</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Metodo per ridurre il bias del modello.
</p>
<ol class="org-ol">
<li>Effettuare un training del dataset;</li>
<li>Trovare i punti che sono classificati erroneamente;</li>
<li>Creare un nuovo dataset tramite sampling in cui i dati classificati erroneamente hanno più probabilità di essere pescati;</li>
<li>Ripetere step 1-3 finché non si ha un'accuratezza desiderata.</li>
</ol>
</div>
</div>
<div id="outline-container-orgac3e57f" class="outline-3">
<h3 id="orgac3e57f"><span class="section-number-3">3.3.</span> Random Forest</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Essenzialmente è un bagging modificato per un Albero di Decisione completo, in modo tale da migliorare la varianza di un modello a bias già basso.
La tecnica porta a un modello molto efficiente in quanto è possibile costruire alberi in parallelo. Il rischio è avere overfitting.
</p>

<p>
Una delle differenze dal bagging tradizionale è che per ogni diverso nodo che va a costruire durante il training, per ogni albero parallelo, l'algoritmo seleziona a caso un subset di feaure ogni volta.
</p>
</div>
</div>
</div>
<div id="outline-container-orgda7a939" class="outline-2">
<h2 id="orgda7a939"><span class="section-number-2">4.</span> Feature Engineering</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org024ab00" class="outline-3">
<h3 id="org024ab00"><span class="section-number-3">4.1.</span> Encoding</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Scelte binarie diventano \(Bool\);</li>
<li>Variabili categoriali possono diventare variabili numeriche crescenti;</li>
<li>Variabili categoriali possono anche essere codificate in One Hot Encoding;</li>
<li>Variabili numeriche sono mappate a \(\mathbb{R}\);</li>
<li>Per valori non esistenti, rimpiazzare con media (numeriche) o mediana (categoriali)</li>
</ul>
</div>
</div>

<div id="outline-container-org4ce966a" class="outline-3">
<h3 id="org4ce966a"><span class="section-number-3">4.2.</span> Misure di accuratezza</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Siano \(\lambda\) label reale, \(\hat{\lambda}\) label predetto, \(c\) una classe:  
</p>
<dl class="org-dl">
<dt>Matrice di confusione (Confusion matrix)</dt><dd>Tabella con \(\lambda\) nelle righe e \(\hat{\lambda}\) nelle colonne;</dd>
<dt>Precisione \(P\)</dt><dd>Rapporto tra numero di predizioni correttamente classificate come \(c\) e  numero di quelle classificate come \(c\);</dd>
<dt>Recall \(R\)</dt><dd>Rapporto tra numero di predizioni correttamente classificate come \(c\) e numero di quelle che sono <span class="underline">realmente</span> in \(c\);</dd>
<dt>F-measure</dt><dd>\[ F = \frac{2PR}{P+R}\]</dd>
</dl>

<p>
È possibile avere diversi tipi di statistiche sui dati:
</p>
<dl class="org-dl">
<dt>Macrostatistica</dt><dd>Media su tutti i dati;</dd>
<dt>Statistica pesata</dt><dd>I pesi sono pesati in base al numero di istanze di una classe;</dd>
<dt>Apprendimento sensibile ai costi</dt><dd>Celle diverse della matrice di confusione hanno errori differenti (ovvero non sono sempre \(\pm 1\)).</dd>
</dl>
</div>
</div>
<div id="outline-container-org45356e3" class="outline-3">
<h3 id="org45356e3"><span class="section-number-3">4.3.</span> ROC e AUC</h3>
<div class="outline-text-3" id="text-4-3">
<p>
ROC sta per curva "Receiver Operating Characteristic".
L'idea è che è preferibile avere un Recall molto alto, che implica un alto numero di "true positives", opposto ad un numero basso di falsi positivi.
</p>

<p>
Quindi, in genere per una selezione binaria, si crea un grafico dove si disegna il True Positive Rate contro il False Positive Rate. \(AOC\), ovvero Area Under the Curve, altro non è che una misura di quanto meglio è. per \(AOC = 1\) si ha la situazione ideale (molto bene), mentre per \(AOC = 0.5\) si ha la situazione di selezione casuale (molto male).
</p>
</div>
</div>
</div>

<div id="outline-container-org8001799" class="outline-2">
<h2 id="org8001799"><span class="section-number-2">5.</span> Text processing</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org9f1bc34" class="outline-3">
<h3 id="org9f1bc34"><span class="section-number-3">5.1.</span> Text encoding</h3>
<div class="outline-text-3" id="text-5-1">
<p>
In genere per processare un documento di testo si può trattare il testo come una sequenza di parole.
Si conta la frequenza delle ricorrenze di ogni parola.
</p>

<p>
Grandezze interessanti:
</p>
<dl class="org-dl">
<dt>Frequenza dei termini nel testo</dt><dd>\(tf(t) = \#\text{ delle occorrenze di t nel testo}\).</dd>
<dt>(no term)</dt><dd>Numero di documenti contenenti un termine::  \(df(t)\).</dd>
<dt>Frequenza inversa nei documenti</dt><dd>\(idf(t) = \ln{\frac{N}{df(t)}}\).</dd>
</dl>

<p>
Si dice stemming il processo di rimozione di suffissi, lemming quello di rimpiazzare una parola composta con la parola primitiva.
</p>
</div>
</div>
<div id="outline-container-org026e67d" class="outline-3">
<h3 id="org026e67d"><span class="section-number-3">5.2.</span> Bayes nativo</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Si utilizza il teorema di Bayes per classificare dei documenti (ad esempio per definire se un messaggio di email è spam oppure no) a partire dal contenuto delle proprie parole.
\[P(Y|X) = \frac{P(Y)P(X|Y)}{P(X)}\]
\[P(C_i|X) = \frac{P(C_i)\prod_jP(x_j|C_i)}{P(X)}\]
In generale si assume indipendenza tra le varie \(P(x_i|Y)\).
</p>

<p>
I valori di \(P(x_j|C_i)\) si possono calcolare in uno dei due modi seguenti:
</p>
<dl class="org-dl">
<dt>Ntive BAyes Multinomiale</dt><dd>\(P(x_j|C_j)\) è il numero di istanze di \(C_i\) la cui feature j-esima ha valore \(X_j\), tutto diviso per la cardinalità di \(C_i\);</dd>
<dt>Native Bayes Gaussiano</dt><dd>Assumere che \(P(x_j|C_i) \sim N(\mu_D, \sigma_D^2)\).</dd>
</dl>

<p>
Onde evitare che l'assenza di un termine provochi il "collasso" del metodo, si applica la coorezione di Laplace, ovvero si aggiunge 1 a tutte le le istanze onde evitare l'esistenza di zeri. Durante il training si salta un valore non trovato, mentre testando si assume una disribuzione \(P(x_j|C_i)\) uguale per ogni \(C_i\).
</p>

<p>
Spesso si trasformano i \(P(C_i|X)\) per motivi di stabilità numerica e computazionale.
</p>
</div>
</div>
</div>
<div id="outline-container-org3c0ec59" class="outline-2">
<h2 id="org3c0ec59"><span class="section-number-2">6.</span> Ranking</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orga32b40c" class="outline-3">
<h3 id="orga32b40c"><span class="section-number-3">6.1.</span> NDCG@K</h3>
<div class="outline-text-3" id="text-6-1">
<p>
"Normalized Discounted Cumulative Gain".
Usato generalmente per dare una misura della qualità dei motori di ricerca.
</p>

<p>
Il principio è cercare i primi \(k\) documenti e sommare assieme il loro contributo (<i>gain</i> \(G(r)\)); più avanti nella pagina stanno, meno significatività hanno (descritto dal termine <i>discount</i> \(D(r)\)).
\[NDCG = \sum_r \frac{2^r - 1}{\log_2(1+r)} = \sum_r G(d_r)\cdot D(r)\]
</p>
</div>
</div>
<div id="outline-container-org2640617" class="outline-3">
<h3 id="org2640617"><span class="section-number-3">6.2.</span> Generazione dei label</h3>
<div class="outline-text-3" id="text-6-2">
<p>
Generare il ranking \(r\) non è facile. Si può
</p>
<ul class="org-ul">
<li>Chiedere agli utenti di dare un voto ai risultati di ricerca</li>
<li>Processare gli input (fissazione, click, riformulazione della ricerca&#x2026;) come indicazione</li>
<li>Tracciare il movimento degli occhi</li>
<li>Chiedere un feedback agli utenti per i primi due risutati</li>
</ul>
</div>
</div>

<div id="outline-container-orgd470942" class="outline-3">
<h3 id="orgd470942"><span class="section-number-3">6.3.</span> PageRank</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Algoritmo usato da Google per determinare un ranking delle pagine web da mostrare.
</p>

<p>
Funziona contando il numero e la qualità dei link a una pagina per stimare quanto importante sia una pagina. L'assunzione è che  siti web più importanti hanno più probabilità di avere link esterni da altri siti web.
</p>
</div>
</div>
</div>

<div id="outline-container-org4157f6c" class="outline-2">
<h2 id="org4157f6c"><span class="section-number-2">7.</span> Artificial Neural Networks</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgd1cca7d" class="outline-3">
<h3 id="orgd1cca7d"><span class="section-number-3">7.1.</span> Neurone artificiale</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Un vero neurone è qualcosa con numerosi input (dendriti) e un solo output (assone).
</p>

<p>
Allo stesso modo un neurone artificiale è una funzione di numerosi input (vettori e matrici)e un solo output. A differenza dei neuroni naturali, questi sono deterministici.
</p>
</div>
<div id="outline-container-org5d881cf" class="outline-4">
<h4 id="org5d881cf"><span class="section-number-4">7.1.1.</span> Apprendimento</h4>
<div class="outline-text-4" id="text-7-1-1">
<ol class="org-ol">
<li>Fai una predizione (Forward)</li>
<li>Misura l'errore (Loss)</li>
<li>Modifica i parametri (Backpropagation)</li>
<li>Ripeti per ogni istanza (Batch)</li>
<li>Ripeti per molte volte (Epochs)</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgaf802de" class="outline-3">
<h3 id="orgaf802de"><span class="section-number-3">7.2.</span> Deep Learning</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Utilizza molti layer nascosti di neuroni tra input e output, utilizzando il metodo <span class="underline">Gradient Descent</span> (con un <span class="underline">learning rate</span>) per trovare un minimo della funzione di <span class="underline">loss</span>, possibilmente un minimo globale. Si possono usare diverse strategie per ottimizzare il processo:
</p>
<dl class="org-dl">
<dt>Stochastic Gradient Descent (SDG)</dt><dd>utilizza metodi di random sampling per la risoluzione del problema del <span class="underline">Gradient Descent</span></dd>
<dt>Metodo dei momenti</dt><dd>Ottimizzazione del layer di neuroni in sé</dd>
</dl>

<p>
Esiste un teorema chiamato <span class="underline">Universal Approximation Theorem</span> che dice che un network composto da trre layer con abbastanza nodi nascosti può approsimare qualsiasi funzione continua.
</p>

<p>
Utilizzando funzioni di attivazione non lineari, il modello riesce a imparare anche funzioni non lineari.
</p>

<p>
La funzione di <span class="underline">loss</span> serve a misurare l'errore tra la predizione e il valore vero. Esistono vari modi in cui definirne una, tra cui:
</p>
<ul class="org-ul">
<li>\(MSE\) o  anche \(MAE\) per problemi di regressione;</li>
<li>Crossentrpy binaria o categorica per problemi di classificazione</li>
</ul>

<p>
I Convolutional Neural Networks (CNN) sono delle reti particolari in cui all'interno sono presenti neuroni che effettuano l'operazione matematica della convoluzione. Sono usati ad esempio nei problemi di classificazione di immagini.
</p>
</div>
</div>
<div id="outline-container-org7ba668f" class="outline-3">
<h3 id="org7ba668f"><span class="section-number-3">7.3.</span> Overfitting</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Quando un Neural Network si adatta troppo ad un dataset, l'accuratezza del training aumenta ma l'accuratezza nel test diminuisce. Succede per varie ragioni:
</p>
<ul class="org-ul">
<li>Il modello è troppo complesso e non ci sono abbastanza dati</li>
<li>Il dataset di training e test non sono campionati correttamente</li>
</ul>

<p>
Pe ril primo caso, se non si può semplificare il modello, occorre aggiungere più dati ed espandere il dataset (data augumentation). Inoltre si possono utilizzare dei dropout per avere dei pesi più generici.
</p>

<p>
Un dropout altro non è che scollegare un certo numero di neuroni a caso dalla rete neurale.
</p>
</div>
</div>
<div id="outline-container-org840bc7e" class="outline-3">
<h3 id="org840bc7e"><span class="section-number-3">7.4.</span> Transfer learning</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Il processo di utilizzare uno o più layer di neuroni su cui è già stato fatto il training in un altro modello, facendo semplicemente un fine-tuning dei parametri in modo tale da poter cambiare obbiettivo in output o anche numero di classi in input.
</p>
</div>
</div>
<div id="outline-container-org10ac054" class="outline-3">
<h3 id="org10ac054"><span class="section-number-3">7.5.</span> 11 Raccomandazioni</h3>
<div class="outline-text-3" id="text-7-5">
<dl class="org-dl">
<dt>Scegliere la misura della qualità all'inizio del lavoro</dt><dd>Cosa mi interessa che classifichi? Quanto accurata?</dd>
<dt>Fissa il processo di valutazione</dt><dd>Training e testing sempre uguali</dd>
<dt>Prepara i dati</dt><dd>In forma matriciale, scegli gli encoding&#x2026;</dd>
<dt>Trova un caso base "ovvio"</dt><dd>Ad esempio la classe più prevalente</dd>
<dt>Trova un caso base migliore</dt><dd>Ad esempio fai una classificazione lineare</dd>
<dt>Costruisci un modello</dt><dd>Stai attento alla funzione di <span class="underline">loss</span> e all'ultimo layer di attivazione (nel dubbio ReLU e rmsprop)</dd>
<dt>Lascia che il modello vada in overfitting</dt><dd>Per essere sicuro che stia effettivamente imparando qualcosa</dd>
<dt>Sistema l'overfitting</dt><dd>rimuovi layer, aggiungi dropout, aggiungi dati, sistema la dimensione della batch&#x2026;</dd>
<dt>&#x2026;</dt><dd>😳</dd>
<dt>Profit!</dt><dd>Il modello funziona</dd>
<dt>Se necessario, crea un modello nuovo da zero</dt><dd>Ci sta rifare da capo</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-org87930f0" class="outline-2">
<h2 id="org87930f0"><span class="section-number-2">8.</span> Clustering</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-orgdfe6040" class="outline-3">
<h3 id="orgdfe6040"><span class="section-number-3">8.1.</span> Obbiettivi e misurazioni del clustering</h3>
<div class="outline-text-3" id="text-8-1">
<p>
L'obbiettivo è raggruppare oggetti simili o relazionati tra loro e oggetti sufficentemente diversi in altri gruppi. L'analisi dei clustering serve a trovare similarità tra i dati.
</p>

<p>
Il clustering è un tipo di unsupervised learning, ovvero un modello che non ha informazioni sulle classi del dataset. Potrebbe essere che si conoscono ma non si vogliono fornire al modello, oppure non si sanno proprio. Può essere usato come una cosa a sé stante oppure come preprocesso ad altri algoritmi.
</p>

<p>
Un buon clustering ha una alta similarità tra gli oggetti della stessa classe e una grande differenza dagli oggetti delle altre classi. La qualità dipende moltissimo dalla qualità della misura della similarità, l'algoritmo e la sua abilità a trovare pattern.
</p>

<p>
Se le classi vere non si conoscono, è difficile misurare la qualità. Inoltre, per spazi ad alte dimensioni, i punti sono più vicini (fenomeno detto <span class="underline">curse of dimensionality</span>).
</p>

<p>
Ci sono molte cose da tenere in considerazione:
</p>
<dl class="org-dl">
<dt>Criteri di partizionamento</dt><dd>Livello singolo o gerarchico</dd>
<dt>Separazione dei cluster</dt><dd>Esclusivo (ad ogni cluster corrisponde una classe) oppure  non esclusivo, (classi multiple con livelli differenti, anche detto <span class="underline">fuzzy</span>)</dd>
<dt>Misure di similitudine</dt><dd>Ad esemipio, basate sulla distanza o sulla connettività dei punti</dd>
<dt>Spazio di clustering</dt><dd>Spazio completo oppure un sottospazio dei dati (ad esempio, usato per dati in alte dimensioni)</dd>
</dl>

<p>
Le distanze possono essere dipendenti (Minkowski) oppure invarianti (coseno, correlazione) rispetto alla scala del problema.
</p>
</div>
</div>
<div id="outline-container-orgfaed48e" class="outline-3">
<h3 id="orgfaed48e"><span class="section-number-3">8.2.</span> Approcci di partizionamento</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Costruisce delle partizioni dei dati e le analissa basandosi su un criterio (chiamato la distanza). La soluzione esatta è un problema \(NP-hard\), quindi spesso si utilizza un approccio euristico: minimizzare una funzione obiettivo (come ad esempio minimizzare \(\sum_{a,b\in C} (d(b) - d(a))^2\), la somma dei quadrati delle distanze all'interno dello stesso cluster \(C\)).
</p>
</div>
<div id="outline-container-org5c4e4a4" class="outline-4">
<h4 id="org5c4e4a4"><span class="section-number-4">8.2.1.</span> k-Means</h4>
<div class="outline-text-4" id="text-8-2-1">
<p>
L'algoritmo consiste in questi passaggi:
</p>
<ol class="org-ol">
<li>Fissare una misura della distanza tra i punti;</li>
<li>Scegli \(k\) punti nello spazio casuali (i "centroidi");</li>
<li>Assegna punti ai \(k\) cluster, in modo tale a minimizzare l'errore (ovvero, assegna punti in base al centroide più vicino);</li>
<li>Sposta i centri che minimizzano l'errore (ovvero, sposta i centroidi nel punto medio dei cluster);</li>
<li>Ripeti dal 3.</li>
</ol>

<p>
La complessità asintitica dell'algoritmo è \(O(knt)\) con \(t\) numero di iterazioni, \(k\) numero di cluster e \(n\) numero di iterazioni, considerato inoltre che \(k << t \vel k << n\).
L'algoritmo è molto efficiente ma è molto sensibile agli outlier e non funziona bene con cluster non sferici e spazi non continui.
</p>
</div>
<ol class="org-ol">
<li><a id="orgb52865a"></a>k-Means++<br />
<div class="outline-text-5" id="text-8-2-1-1">
<p>
Una variazione dell'algoritmo k-Means,  dove nella fase iniziale scegli il primo centro random, scegliendo i rimanenti a distanza proporzionalmente crescente rispetto al centro più vicino (più distante è più probabile).
</p>
</div>
</li>
<li><a id="orgc2dc17a"></a>k-Medioids<br />
<div class="outline-text-5" id="text-8-2-1-2">
<p>
L'algoritmo è uguale al k-Means, solo che si utlizza un centro che è anche un punto del dataset; è un modo per renderlo più resistente agli outliers.
</p>
</div>
</li>
<li><a id="orgfd7857a"></a>Grafico a gomito (Elbow plot)<br />
<div class="outline-text-5" id="text-8-2-1-3">
<p>
Strumento per determinare quale sia il valore di \(k\) ottimale da utilizzare. Si sceglie il \(k\) dopo il quale la differenza tra gli \(SSE\) non è vantaggiosa (ovvero "sul gomito" del grafico).
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org697f449" class="outline-3">
<h3 id="org697f449"><span class="section-number-3">8.3.</span> Approcci gerarchici</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Si crea una decomposizione gerarchica del dataset per poi selezionare un certo numero di tagli del dataset. Anche questo metodo usa una distanza o una matrice di similarità.
</p>

<p>
L'approccio può essere agglomerativo (costruisce, bottom up) oppure divisivo (taglia, top down).
</p>
</div>

<div id="outline-container-org408bec7" class="outline-4">
<h4 id="org408bec7"><span class="section-number-4">8.3.1.</span> Hierarchical Agglomerative Cluster</h4>
<div class="outline-text-4" id="text-8-3-1">
<ol class="org-ol">
<li>Inizializza ogni punto come se fosse un cluster a sé stante;</li>
<li>Unisce i cluster più vicini in termnini di distanza o similitudine;</li>
<li>Ripeti finché non c'è un solo cluster</li>
</ol>

<p>
L'algoritmo è chiaramente agglomerativo. La sua complessità  è \(O(n^3)\) per la soluzione <span class="underline">naive</span>, ma può scendere a \(O(n^2\log n)\) utilizzando un <span class="underline">min-heap</span> e a \(O(n^2)\) con un <span class="underline">single linkage</span>.
</p>

<p>
Gran parte dell'ottimizzazione avviene nella scelta della distanza da computare tra i diversi clusters (anche detta <span class="underline">linkage</span>):
</p>
<dl class="org-dl">
<dt>Single lnkage</dt><dd>Distanza minima, spesso sovrastima la similiraità;</dd>
<dt>Complete linkage</dt><dd>Distanza massima, spesso sottostima la similarità;</dd>
<dt>Average linkage</dt><dd>Distanza media, similarità più ottimale;</dd>
<dt>Centroid o Medioid linkage</dt><dd>Come il caso average linkage, ma molto più efficiente;</dd>
<dt>Ward linkage</dt><dd>Utilizza l'incremento della somma dei quadrati degli errori.</dd>
</dl>

<p>
Per vedere come i cluster si agglomerano nel tempo, si utlizza un diagramma particolare chiamato <span class="underline">dendogramma</span>, che disegna un albero mostrando ad ogni step come sono suddivisi i cluster.
</p>
</div>
</div>
</div>
<div id="outline-container-org769c6a7" class="outline-3">
<h3 id="org769c6a7"><span class="section-number-3">8.4.</span> Approcci di densità</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Questi approcci si basano sull'utilizzo di una funzione di densità e sulla connettività tra i punti. Il vantaggio è l'arbitrarietà delle forme dei cluster e una minore suggestività agli outlier.
</p>
</div>
<div id="outline-container-orgcf8127e" class="outline-4">
<h4 id="orgcf8127e"><span class="section-number-4">8.4.1.</span> DBSCAN</h4>
<div class="outline-text-4" id="text-8-4-1">
<p>
<span class="underline">Densitiy Based Spatial Clustering of Applications with Noise</span>.
</p>

<p>
L'algoritmo seleziona i punti nelle regioni più dense e le unisce, scartando gli outlier.
Definizioni varie:
</p>
<ul class="org-ul">
<li>Un punto è detto <span class="underline">core point</span> se ci sono almeno \(m\) punti con una distanza minore di \(\varepsilon\) dal punto</li>
<li>Un punto \(q\) è <span class="underline">direclty density reachable</span> da \(p\) se \(p\) è un <span class="underline">core point</span> e \(d(p,q)<\varepsilon\);</li>
<li>Un punto \(q\) è <span class="underline">densitiy reachable</span> da \(p\) se esiste una successione di punti \(p_1, \dots, p_n\) che sono <span class="underline">directly density reachable</span> in successione tra loro;</li>
<li>Un punto \(q\) è <span class="underline">density connected</span> a \(p\) se esiste un punto \(o\) tale che enteambi \(p,q\) sono <span class="underline">density reachable</span> da o.</li>
<li>Un cluster è un insieme di punti <span class="underline">density connected</span>.</li>
</ul>

<p>
L'algoritmo ha complessità \(O(n \log n)\) se si utilizza un indicizzazione spaziale.
</p>

<p>
Pro: L'algoritmo è poco sensibile al rumore e permette di avere cluster di forma qualsiasi, non solo circolare.
</p>

<p>
Contro: L'algoritmo è più complesso nel senso che ci sono due iperparametri - Il numero minimo di punti \(m\) e il raggio \(\varepsilon\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgaee86ad" class="outline-3">
<h3 id="orgaee86ad"><span class="section-number-3">8.5.</span> Valutazione dei clsuter</h3>
<div class="outline-text-3" id="text-8-5">
</div>
<div id="outline-container-orge1514d6" class="outline-4">
<h4 id="orge1514d6"><span class="section-number-4">8.5.1.</span> Intrinseca</h4>
<div class="outline-text-4" id="text-8-5-1">
<p>
Non si sanno label e classi dei cluster, quindi è necessario valutare quanto simili sono i cluster tra loro e diversi dagli altri
</p>
<dl class="org-dl">
<dt>Intra-class similarity</dt><dd>\(a(o_i) = \sum_C \frac{d(o_i, o_j)}{|C| - 1}\).</dd>
<dt>Inter-class similarity</dt><dd>\(b(o_i) = \min_C\sum_C \frac{d(o_i, o_j)}{|C|}\).</dd>
<dt>Coefficiente di silhouette</dt><dd>\(s(o_i) = \frac{b(o_i) - a(o_i)}{max(a(o_i), b(o_i))}\). Nota che \(s(o_i) \in [-1,1]\). Più alto è, meglio è</dd>
</dl>
</div>
</div>
<div id="outline-container-org9ff2745" class="outline-4">
<h4 id="org9ff2745"><span class="section-number-4">8.5.2.</span> Estrinseca</h4>
<div class="outline-text-4" id="text-8-5-2">
<p>
Il valore dei label e classi è conosciuto.
</p>

<dl class="org-dl">
<dt>Tabella di contingenza</dt><dd>True Negative \(T_-\), True Positive \(T_+\), False Negative \(F_-\), False Positive \(F_+\).</dd>
<dt>Statistica rand</dt><dd>\(RAND = \frac{T_+ + T_-}{T_+ + T_- + F_+ + F_-}\).</dd>
<dt>Coefficiente di Jaccard</dt><dd>\(JACC = \frac{T_+}{T_+ + F_+ + F_-}\)</dd>
</dl>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Duskhorn</p>
<p class="date">Created: 2024-01-22 Mon 18:24</p>
</div>
</body>
</html>